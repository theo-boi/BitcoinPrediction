{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Binôme : EZRA Seunkam et BOISSEAU Théo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Définition du sujet\n",
    "\n",
    "### Le cours du Bitcoin\n",
    "\n",
    "Dans les fichiers Cotations2021.csv et Cotations2020.csv se trouvent l'évolution journalière du cours du Bitcoin en 2021 et en 2020. Ces fichiers contiennent 5 colonnes :\n",
    " - Date : jour de la cotation,\n",
    " - Ouverture : valeur en euros du cours du Bitcoin à l'ouverture du marché,\n",
    " - Plus Haut : valeur en euros la plus élevée du Bitcoin durant le jour considéré,\n",
    " - Plus Bas : valeur en euros la plus basse du Bitcoin durant le jour considéré,\n",
    " - Clôture : valeur en euros du cours du Bitcoin à la clôture du marché.\n",
    "\n",
    "La notion de \"cours du Bitcoin\" est assez vague : finalement, on se pose vraiment la question de savoir combien vaut le Bitcoin au moment où on doit en acheter ou en vendre... Pour simplifier un peu, nous considérerons que d'un jour $t$ à un jour $(t+1)$ le cours du Bitcoin augmente (diminue) si sa valeur à la clôture au jour $t$ est inférieure (supérieure) à sa valeur à la clôture au jour $(t+1)$. \n",
    "\n",
    "Autrement dit, on va regarder la colonne \"Clôture\" pour décider de l'augmentation ou la baisse du cours du Bitcoin. \n",
    "\n",
    "### Enoncé du problème de classification\n",
    "\n",
    "Le problème que nous avons à résoudre est donc un problème de classification : pour un jour $t$ donné, nous voulons prédire si à $(t+1)$ le cours va augmenter (classe 1) ou diminuer (classe 2). Le cas, marginal, ou le cours reste constant sera assimilé à la classe 2.\n",
    "\n",
    "Pour réaliser cette prédiction, nous pouvons prendre en compte l'historique du cours sur les $h$ derniers jours. Quelle valeur de $h$ considérer ? Quelle(s) donnée(s) considérer sur les jours précédents : uniquement la valeur de clôture ou ... ?\n",
    "\n",
    "### Démarche\n",
    "\n",
    "1. Définir la notion d'historique : quel vecteur de données allons nous prendre en compte pour l'apprentissage et la prédiction ensuite ? Notons $\\vec v$ ce vecteur.\n",
    "1. Définir le problème d'optimisation à résoudre (fonction $H(\\vec v)$ de séparation des données, fonction d'activation (sigmoide), fonction de perte/erreur),\n",
    "2. Définir le gradient,\n",
    "3. Créer notre algorithme de descente,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Développement de notre algorithme de descente\n",
    "\n",
    "### Outils\n",
    "- La bibliothèque [pandas](https://pandas.pydata.org/docs/user_guide/index.html) est idéale pour l'utilisation d'une base de donnée.\n",
    "- La bibliothèque [numpy](https://numpy.org/doc/1.18/user/index.html) est parfaite pour tout calcul sur des vecteurs.\n",
    "- La bibliothèque [matplotlib](https://matplotlib.org/contents.html) est la base pour visualiser des résultats mais accouplé à [seaborn](https://seaborn.pydata.org/index.html) il est possible d'aller plus vite.\n",
    "- Le bibliothèque [math](https://docs.python.org/fr/3.9/library/math.html) est souvent oubliée tellement elle semble évidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import math\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lecture des données\n",
    "\n",
    "La première tâche à réaliser va être de développer une fonction qui va lire un fichhier .csv contenant un historique de cotations et créer une base panda. La structure $RawData$ va contenir les données telles que contenues dans le fichier .csv passé en paramètre (selon la syntaxe décrite précédemment). De même, $RawC$ va contenir l'appartenance aux classes 1 (diminution) / 0 (augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Input: nom du fichier .csv à lire.\n",
    "#Nécessite :les lignes du fichier sont classées par dates de cotatation croissantes et les colonnées séparées par \";\"\n",
    "#Output: RawData, contient l'historique des cotations telles que lues dans le .csv\n",
    "#        RawC, contient pour chaque ligne/date t (sauf la dernière) 1 si le cours baisse à t+1; 0 sinon.\n",
    "def LireDonnees(NomFichier):\n",
    "    #Lecture des données dans le fichier csv\n",
    "    RawData=pd.read_csv(NomFichier, sep=';')\n",
    "    \n",
    "    #Construction des classes d'appartenance de chaque ligne de la base: on suppose que les cotations journalières sont\n",
    "    #classées par ordre chronologique\n",
    "    RawC=[]\n",
    "    for index in range(len(RawData)-1):\n",
    "        value=RawData.iloc[index,4] > RawData.iloc[index+1,4]\n",
    "        value=1*value\n",
    "        RawC.append(value)\n",
    "    return RawData, RawC\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Définition des données utilisées pour l'apprentissage/prédiction\n",
    "\n",
    "Nous allons maintenant définir une méthode qui va préparer une structure de données, nommée $data$, qui va contenir pour chaque ligne un couple :\n",
    "\n",
    "$\\vec v=(\\vec x ; c_x)$\n",
    "\n",
    "où $\\vec x$ est le vecteur caractérisant la ligne en cours. Par exemple, pour la ligne $t$, date de cotation $t$, ce qui explique la prédiction $c$ (baisse/augmentation de la valeur de la cotation à $t+1$) c'est le cours du Bitcoin à la clôture sur les $h=3$ derniers jours. Donc, la ligne d'indice 0 dans $data$ va contenir :\n",
    "\n",
    "$(x_1,x_2,x_3, c_x=0$ ou $1)$ \n",
    "\n",
    "avec $x_j$ la valeur de cotation à la clôture le jour $j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def EquilibrerData(RawData,RawC):\n",
    "    # On crée un nouveau dataset équilibré\n",
    "    RawDataEquilibre = RawData.copy()\n",
    "    RawCEquilibre = RawC.copy()\n",
    "\n",
    "    Cdf = pd.DataFrame(RawC)\n",
    "    zero = Cdf.loc[Cdf[0] == 0] # On identifie les 0 de RawC\n",
    "    un = Cdf.loc[Cdf[0] == 1] # On identifie les 1 de RawC\n",
    "\n",
    "    # On cible les lignes du dataset qu'il faut enlever pour avoir autant de 0 que de 1\n",
    "    indexASupprimer = []\n",
    "    if len(zero) > len(un):\n",
    "        indexASupprimer = zero.sample(n=len(zero.index) - len(un.index)).index.tolist()\n",
    "    if len(un) > len(zero):\n",
    "        indexASupprimer = un.sample(n=len(un.index) - len(zero.index)).index.tolist()\n",
    "\n",
    "    # On effectue la suppression\n",
    "    if indexASupprimer != []:\n",
    "        indexASupprimer.sort()\n",
    "        RawDataEquilibre.drop(index=indexASupprimer, inplace=True)\n",
    "        RawDataEquilibre.reset_index(drop=True, inplace=True)\n",
    "        for iterateur in range(len(indexASupprimer)):\n",
    "            RawCEquilibre.pop(indexASupprimer[iterateur] - iterateur)\n",
    "\n",
    "    return RawDataEquilibre, RawCEquilibre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Input: RawData et RawC ont été créés à partir d'un fichier .csv\n",
    "#Output: D, contient les points $(\\vec x)$. Cela doit être une matrice numpy\n",
    "#        C, contient la classe d'appartenance $c_x$ de chaque entrée dans data. Cela doit être un vecteur numpy\n",
    "def CreerData(RawData,RawC):\n",
    "    (RawDataEquilibre,RawCEquilibre) = EquilibrerData(RawData,RawC) # On équilibre le dataset pour avoir autant de 0 que de 1\n",
    "\n",
    "    h = 6\n",
    "    pas = 3\n",
    "    variables = (\"Ouverture\", \"Plus Haut\", \"Plus Bas\", \"Cloture\") # On sélectionne les variables désirées\n",
    "    std = RawDataEquilibre.std(axis=1,numeric_only=True) # On calcule l'écart type à chaque date\n",
    "    moy = RawDataEquilibre.mean(axis=1,numeric_only=True)  # On calcule l'espérance à chaque date\n",
    "\n",
    "    nbDates = len(RawDataEquilibre.index) - h*pas\n",
    "    D = np.empty((nbDates, h*len(variables)))\n",
    "    for t in range(nbDates): # On va instancier chaque vecteur x\n",
    "        for p in range(h): # En écrivant le point x_(t+p)\n",
    "            for pVar in range(len(variables)): # et pour chacun, chacune de ses variables\n",
    "                # On normalise de façon à centrer normer chaque donnée d'un même jour.\n",
    "                D[t][pVar + p*len(variables)] = (RawDataEquilibre[variables[pVar]][t+p*pas] - moy[t+p*pas]) / std[t+p*pas]\n",
    "    if -h*(pas-1) != 0 :\n",
    "        C = np.array(RawCEquilibre)[h-1:-h*(pas-1)]\n",
    "    else:\n",
    "        C = np.array(RawCEquilibre)[h-1:]\n",
    "\n",
    "    return(D,C)\n",
    "\n",
    "(RawData,RawC)=LireDonnees(\"Cotations2020.csv\")\n",
    "(D,C)=CreerData(RawData,RawC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Définition du problème d'optimisation à résoudre\n",
    "\n",
    "On a choisi\n",
    "$\\vec v=(\\vec x ; c_x) \\in R ^{ 4h +1}, \\ avec \\ \\forall i \\in \\{1..h\\}, \\ \\vec x_i \\in R^4$ étant l'ensemble des valeurs des variables sélectionnées du point $\\vec x_i$\n",
    "\n",
    "$H(\\vec v) = \\displaystyle\\sum_{i=1}^{h} \\vec w_i \\cdot \\vec x_i + b, \\ avec \\ h = 3 \\ et \\ \\forall i \\in \\{1..h\\}, \\ \\vec w_i \\in R^4$\n",
    "\n",
    "D'où $H(\\vec v) = \\vec w_1 \\cdot \\vec x_1 + \\vec w_2 \\cdot \\vec x_2 + \\vec w_3 \\cdot \\vec x_3 + b$\n",
    "\n",
    "On concatennera les $\\vec x$ pour chaque $\\vec v$ dans la matrice D, les $c_x$ seront stockés dans le vecteur colonne C, et on concatennera les $\\vec w_i$ dans le vecteur weights.\n",
    "\n",
    "On va utiliser la fonction logistique sigmoide pour transformer les valeurs de $H(\\vec v)$ en valeurs $0$ ou $1$:\n",
    "\n",
    "$\\mathcal{S}: x \\mapsto \\frac{1}{1+\\exp^{-\\lambda x}}$.\n",
    "\n",
    "Notre classificateur est donc : $\\mathcal{S}(H(\\vec v))$.\n",
    "\n",
    "Pour notre problème, on cherche à apprendre sur la base d'apprentissage les valeurs de $\\vec w_i \\ \\forall i \\in \\{1..h\\}$ et $b$.\n",
    "\n",
    "On choisit la cross-entropy pour mesurer l'erreur de notre problème, c'est à dire la distance entre $\\mathcal{S}(H(\\vec v))$ et $c_{\\vec v}$ :\n",
    "\n",
    "$E(\\mathcal{P}) = \\sum \\limits_{\\vec v \\in \\mathcal{P}}-(c_{\\vec v}\\log_2(\\mathcal{S}(H(\\vec v)))+(1-c_{\\vec v})\\log_2(1-\\mathcal{S}(H(\\vec v))))$.\n",
    "\n",
    "$\\mathcal{P} : \\Bigg\\{$\n",
    "$\\  Minimiser \\ E(P)$\n",
    "$\\ \\ s.c.$\n",
    "$\\ \\ \\vec w_i \\in R^4 \\ et \\ b \\in R$\n",
    "$\\Bigg\\}$\n",
    "\n",
    "Commençons par définir le code qui calcule $H(\\vec v)$ puis le code de la fonction logistique sigmoid $\\cal{S}(x)$. Je suppose que vous allez reprendre les mêmes fonctions que dans le TP introductif, mais si vous voulez changer vous pouvez, bien sûr ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# définition de la méthode pour calculer H(v)\n",
    "def H(weights, constant, point):\n",
    "    value=np.sum(weights*point)+constant\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Définissons maintenant le code qui calcule $\\mathcal{S}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# définition de la fonction sigmoide (logistique) et la fonction d'arrondie\n",
    "def Sigmoid(x, lamda=0.001):\n",
    "        return 1/(1+np.exp(-lamda*x))\n",
    "\n",
    "\n",
    "def Step_function(x):\n",
    "        return 1*(x>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Passons maintenant à la définition de la fonction d'erreur $E(\\mathcal{P})$, qui est notre fonction à minimiser, ainsi que du gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# définition de la méthode pour calculer l'erreur \n",
    "#Input : weights, le vecteur de poids utilisés dans la fonction H,\n",
    "#        constant, la constante utilisée dans la fonction H\n",
    "#Output : une valeur numérique, l'erreur commise. \n",
    "def Loss_function(weights, constant):\n",
    "    loss_value = 0\n",
    "    for index in range(len(D)):\n",
    "        p = D[index:index+1,:]\n",
    "        c = C[index]\n",
    "        loss_value -= c*np.log2(Sigmoid(H(weights, constant, p)))+(1-c)*np.log2(1-Sigmoid(H(weights, constant, p)))\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# définition du calcul de gradient\n",
    "#Input : weights, le vecteur de poids utilisés dans la fonction H,\n",
    "#        constant, la constante utilisée dans la fonction H\n",
    "#Output : le gradient, un numpy array\n",
    "def Gradient(weights, constant):\n",
    "    grad = []\n",
    "    # Looping over all variables (number of variables=number of weights)\n",
    "    for w in range(len(weights)):\n",
    "        coord_value=0\n",
    "        for index in range(len(D)):\n",
    "            x=D[index:index+1,:]\n",
    "            coord_value-= x[0,w]*(C[index]-Sigmoid(H(weights, constant, x)))\n",
    "        grad.append(coord_value)\n",
    "    coord_value=0\n",
    "    # Considering the last component=constant\n",
    "    for index in range(len(D)):\n",
    "        x=D[index:index+1,:]\n",
    "        coord_value-= (C[index]-Sigmoid(H(weights, constant, x)))\n",
    "    grad.append(coord_value)\n",
    "    return np.array(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Algorithme de descente\n",
    "\n",
    "Avant de procéder à l'apprentissage, nous avons dans un premier temps équilibré la base (pour avoir autant de \"0\" que de \"1\"), puis centré et réduit les données date par date, sinon l'algorithme prédisait 100% de zéros ou 100% d’uns. En ayant testé plusieurs combinations de h (la fenêtre de jours) et de p (le pas entre les jours), on a trouvé que **h = 6** jours (à peu près 1 semaine) et **pas = 3** jours marchaient le mieux.  En prenant les h=3 derniers jours comme c’était le cas au début, on n’avait pas beaucoup d’information sur la période dans laquelle le jour à prédire était situé.\n",
    "\n",
    "Avec cette base d’apprentissage, et en prenant **1000 itérations** et un **Learning rate de 1.2**, on obtient de meilleurs résultats. Après la normalisation des données, on a remarqué que le pas était devenu trop petit à cause du gradient qui avait beaucoup diminué, donc on a choisi un Learning rate beaucoup plus grand pour l’augmenter. En dessous de 1000 itérations, on n’apprend pas suffisamment et donc l’erreur est encore élevée, et au-delà de ça, l’algorithme ne peut pas très bien se généraliser pour prédire sur une nouvelle base.\n",
    "\n",
    "Enfin, avoir un **lamda de 0.001** par rapport à la norme de notre gradient nous permet d'avoir des valeurs de la sigmoide proches de 0.5. Cela permet de faire évoluer les poids plus facilement lors du calcul du gradient et de mieux estimer l'erreur, ce qui permet également d'améliorer les prédictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# L'algorithme de gradient qui va travailler à partir des structures numpy D et C\n",
    "# Input :\n",
    "# Nécessite :\n",
    "# Output : weights, un numpy array contenant les poids utilisés dans la fonction de séparation H \n",
    "#          constant, un numérique contenant la constante utilisée dans la fonction de séparation H\n",
    "#          nbstep, nombre d'itérations mises par l'algorithme avant de s'arrêter\n",
    "\n",
    "def Algorithme_Gradient():\n",
    "    \n",
    "   #Initialisations pour les graphiques terminaux: à conserver\n",
    "    ## Produire deux graphes cote à cote 2 subplots\n",
    "    fig, ax = plt.subplots(figsize=(10, 5) , nrows=1, ncols=1)\n",
    "    ## Pour visualiser les evolutions en fonction du step\n",
    "    itersteps = []\n",
    "    ## Pour visualiser l'evolution de la norme du gradient\n",
    "    iterGap = []\n",
    "    ## Pour visualiser l'évolution de l'erreur\n",
    "    iterError = []\n",
    "    ## Pour visualiser la séparation linéaire\n",
    "    #line_X = np.array([min(data[\"x\"]), max(data[\"x\"])])\n",
    "    \n",
    "    #Initialisation de l'algorithme du gradient \n",
    "    ## initialise les poids et la constante\n",
    "    weights = np.random.random(np.shape(D)[1])\n",
    "    constant = rd.random()\n",
    "    ## initialise le pas d'optimisation\n",
    "    learning_rate = 1.2\n",
    "    #Initialisation du pas d'itération\n",
    "    nbstep = 1000\n",
    "    step = 1 # numéro d'itération dans l'algorithme\n",
    "\n",
    "    # Exécution de l'algorithme de gradient\n",
    "    while step < nbstep:\n",
    "        \n",
    "        # Calcul de la fonction de perte/erreur\n",
    "        E = Loss_function(weights,constant)\n",
    "\n",
    "        # Calcul du gradient et de sa norme\n",
    "        total_grad = Gradient(weights,constant)\n",
    "\n",
    "        norm = np.linalg.norm(total_grad)\n",
    "        # Mise à jour des poids et de la constante\n",
    "        weights-=(learning_rate*total_grad[:-1])/norm\n",
    "        constant-=learning_rate*total_grad[-1]/norm\n",
    "\n",
    "        # Stockage de valeurs à cette itération pour affichage final\n",
    "        itersteps.append(step - 1)\n",
    "        iterGap.append(norm)\n",
    "        iterError.append(E)\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "    # Affichage final : à conserver\n",
    "    ax.plot(itersteps,iterGap, label='Norme du gradient')\n",
    "    ax.plot(itersteps,iterError, label='Erreur')\n",
    "    ax.set_xlabel('step')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return(weights,constant,nbstep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Phase d'apprentissage \n",
    "\n",
    "* La fonction LireDonnees() doit être appelée pour lire des données brutes dans un fichier .csv qui est notre base d'apprentissage,\n",
    "* La fonction CréerData() doit être appelée pour créer les structures $D$ et $C$ qui sont utilisées par notre algorithme de gradient,\n",
    "* L'algorithme de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAE9CAYAAADJfiwbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6wklEQVR4nO3deXydZZ3//9cnJ/vSpFm7pCEtXaALLW1ZStlkExBB/YrIzLDpWHBgUPT7VXT8CjPjd346MjDqjKOMyqJMxQUQFRCKICCbbSktXSila9qkaZI2W5ukSa7fH9d9ck62Nm1Ock6S9/PxuB/nnPu+zznX6eG0bz7XdV+XOecQERERkcFLincDREREREYLBSsRERGRGFGwEhEREYkRBSsRERGRGFGwEhEREYkRBSsRERGRGEmOdwMACgsLXXl5ebybISIiInJUq1atqnHOFfV1LCGCVXl5OStXrox3M0RERESOysx29HdMXYEiIiIiMaJgJSIiIhIjClYiIiIiMZIQY6xEREQSzeHDh6moqKClpSXeTZE4SU9Pp7S0lJSUlAE/R8FKRESkDxUVFeTk5FBeXo6Zxbs5Msycc9TW1lJRUcHUqVMH/Dx1BYqIiPShpaWFgoIChaoxyswoKCg45oqlgpWIiEg/FKrGtuP5/hWsREREEpSZ8cUvfrHr8T333MPdd98dvwZF2b59O3Pnzo1rG2688UZ+9atfAfC3f/u3bNiw4bhe58UXX+TVV1+NSZsUrERERBJUWloajz32GDU1Ncf1/Pb29hi3aOgdb5t/9KMfMXv27ON6roLV8Xjjh1C9Kd6tEBERGbDk5GSWLVvGfffd1+vY9u3bueCCCzjllFO48MIL2blzJ+CrOLfccgtnnHEGX/rSl7jxxhv57Gc/y5lnnsm0adN48cUX+dSnPsXJJ5/MjTfe2PV6zz77LEuWLGHhwoVcffXVNDU19XrPVatWMX/+fObPn89//ud/du1/8MEHue2227oeX3HFFbz44ou9nv/UU09x0kknsWjRIm6//XauuOIKAO6++26uu+46li5dynXXXcf27ds555xzWLhwIQsXLuwKPc45brvtNmbNmsVFF11EdXV112uff/75Xau49PdZysvLueuuu1i4cCHz5s1j06ZNbN++nR/84Afcd999LFiwgJdffnmgX0+fxkawOrALnrkTvn8GfH8JvPRtqH0/3q0SERE5qltvvZVHHnmE+vr6bvv//u//nhtuuIG1a9fy13/919x+++1dxyoqKnj11Ve59957Adi/fz+vvfYa9913H1deeSV33HEH69evZ926daxZs4aamhq+8Y1vsGLFClavXs3ixYu7nhvtpptu4nvf+x5vv/32MX+OlpYWbr75Zp5++mlWrVrFvn37uh3fsGEDK1asYPny5RQXF/Pcc8+xevVqHn300a7P9vjjj/Puu++yYcMGHn744T6rTEf7LIWFhaxevZrPfvaz3HPPPZSXl3PLLbdwxx13sGbNGs4555xj/mzRxsZ0C3lT4AubYMNvYP1j8Mdv+G3ifJj7v2DORyGvLN6tFBGRBPWPv13Phj0NMX3N2ZPGcdeH5xz1vHHjxnH99dfz3e9+l4yMjK79r732Go899hgA1113HV/60pe6jl199dWEQqGuxx/+8IcxM+bNm0dJSQnz5s0DYM6cOWzfvp2Kigo2bNjA0qVLAWhra2PJkiXd2nHgwAEOHDjAueee2/WeTz/99IA/76ZNm5g2bVrX1AXXXnst999/f9fxK6+8suvzHT58mNtuu401a9YQCoXYvHkzAC+99BLXXnstoVCISZMmccEFF/R6n9dff/2In+VjH/sYAIsWLer684ulsRGsAHJK4IxlfquvgPVPwDu/hue+7rfS03zImv0RGDcx3q0VERHp8vnPf56FCxdy0003Dej8rKysbo/T0tIASEpK6rofftze3k4oFOLiiy9m+fLlx9W+5ORkOjs7ux4fz6Sq0W2+7777KCkp4e2336azs5P09PQBv45z7oifJfz5Q6HQkIxBGzvBKlpuKZx1m9/qtsH6x30l65k74ZmvwAlnwdyPwclXQXZRvFsrIiJxNpDK0lDKz8/nE5/4BD/+8Y/51Kc+BcBZZ53Fz3/+c6677joeeeSRQXVhnXnmmdx6661s2bKF6dOn09zczO7du5k5c2bXOXl5eeTl5fHKK69w9tln88gjj3QdKy8v5/vf/z6dnZ3s3r2bN998s9d7zJo1i61bt7J9+3bKy8t59NFH+21PfX09paWlJCUl8dBDD9HR0QHAueeeyw9/+ENuuOEGqqureeGFF/irv/qrY/4sPeXk5NDQEJuK5FHHWJnZFDN7wcw2mNl6M/tcsP/bZrbJzNaa2eNmlhfsLzezQ2a2Jth+EJOWDpX8qXDOF+CWV+C2lXD+V6C5Bn7/Rfi3mfDwVbD6YThYF++WiojIGPbFL36x29WB3/ve93jggQc45ZRT+OlPf8p3vvOd437toqIiHnzwQa699lpOOeUUlixZwqZNvS/4euCBB7j11ltZsGABzrmu/UuXLmXq1KnMnj2b22+/nYULF/Z6bkZGBt///ve59NJLWbRoETk5OeTm5vbZnr/7u7/joYceYv78+WzatKmrmvXRj36UGTNmMHv2bK6//vpe3ZXH8lmiffjDH+bxxx+PyeB1i/6D6fMEs4nAROfcajPLAVYBHwFKgT8659rN7FsAzrkvm1k58Dvn3IAnt1i8eLELj+RPCM5B9QZ45zHfXbh/GyQlw4kX+O7CWZdD+rh4t1JERIbQxo0bOfnkk+PdjFGlqamJ7OxsnHPceuutzJgxgzvuuCPezTqivv47MLNVzrnFfZ1/1K5A51wlUBncbzSzjcBk59yzUae9Dnz8uFudaMygZI7fLvgaVK7xIWv94/D4zRBKgxkX++7CmZdCatZRX1JERGSs++///m8eeugh2traOPXUU7n55pvj3aSYO2rFqtvJvhr1EjDXOdcQtf+3wKPOuZ8F56wHNgMNwNecc0esqyVcxao/nZ2we6WvYq1/ApqqICUTZn7QD3qfcbFClojIKKGKlcAQVKyiXiQb+DXw+R6h6h+AdiA8iq0SKHPO1ZrZIuAJM5sT/ZzgecuAZQBlZSNkqoOkJJhyut8++C+w8zUfsjb+1lezUjJ9uJr9ER+2FLJERETGlAEFKzNLwYeqR5xzj0XtvxG4ArjQBaUv51wr0BrcX2Vm7wMzgW4lKefc/cD94CtWg/4kwy0pBOVn++3ye2DHn30Va+Nv/XxZyRkw46IgZF0KadnxbrGIiIgMsaMGK/NLO/8Y2Oicuzdq/6XAl4DznHMHo/YXAXXOuQ4zmwbMALbGvOWJJCkEU8/12+Xf9pWs9U/Axid90EpOh+lByJp1KaTlxLvFIiIiMgQGUrFaClwHrDOzNcG+rwLfBdKA53z24nXn3C3AucA/mdlhoBO4xTk3duYqiK5kXfYt2PWGD1kbfgObfucHvk+/COZ8xFeydHWhiIjIqDGQqwJfAayPQ0/1c/6v8d2GkhTyk42ecBZc+k0fsjb8xm/v/h5CqXDihZGQlZEX7xaLiEgCCYVCXcvPAHzyk5/kzjvvjGOL5GjG5szr8ZCUBCcs8dsH/wUq/gIbnvAha/PTkJTi58ma8xE/T5ZClojImJeRkcGaNWuOeE5HR0e3dQF7Pj5W7e3tJCcrHhyvo868LkMgKQnKzoBL/z/4/Dvw6RVwxs1+UtInPgvfng6PXA1v/UwzvouISC/l5eV8+ctfZuHChfzyl7/s9fjZZ59lyZIlLFy4kKuvvpqmpqau54Vnb1+5ciXnn38+AHfffTfXXXcdS5cu5brrrovXxxoVFEnjLSkJppzmt0u+AbtXw4bHYf1v4L1n/YzvU8+Dkz8MJ12htQtFRMaQQ4cOsWDBgq7HX/nKV7jmmmsAKCgoYPXq1QDceeedXY9ramr42Mc+xooVK8jKyuJb3/oW9957L1//+teP+F4bNmzglVdeISMjY8g+z1igYJVIzKB0kd8u/mfY81bQXfgk/O7z8PsvQNkSH7JO/rBfTFpERIbe03dC1brYvuaEeXDZN494ypG6AsMBq+fj119/nQ0bNrB06VIA2tra+lxTr6crr7xSoSoGFKwSlRlMXui3i/4R9q73UzdsfBKeudNvkxb6gDX7Kig4Md4tFhGRYRRemLjnY+ccF198McuXL+/1nOTkZDo7OwFoaWk54uvJ8VGwGgnMYMJcv33gK1CzBTb91leynv9HvxXPDipZV/o1Dq2vCzlFROS4HKWylEjOPPNMbr31VrZs2cL06dNpbm5m9+7dzJw5k/LyclatWsVll13Gr3+tC/iHggavj0SF0+HsO2DZC37w+6Xfgozx8Kd/hR8she+eCs/+X6hY6dc3FBGRESk8xiq8DWSqhaKiIh588EGuvfZaTjnlFJYsWcKmTZsAuOuuu/jc5z7H4sWLB3XloPTvmBZhHiojZhHmRNdUDe8+5StZ2/4Ene2QMwlOvsJXssqWQEhFShGRgdAizAJDuAizjADZxbDoRr8dOgCb/+DHZK1+GN68HzIL/BxZs6/yy+8kp8W5wSIiIqOLgtVolZEH86/xW1szbFnhK1nrn4C3fgpp42DmB30la/qFkKpBiyIiIoOlYDUWpGb5KtXsq6C9Fbb+CTb+BjY9Bet+CckZftb3kz7kl9bJKoh3i0VEREYkBauxJjkNZl7ityvaYeerfhqHTU/59QstCcrOgpMu90FrfHm8WywiEjfOOUxXWY9ZxzMOXYPXxXMOKt+GTb/3W/V6v79krg9YJ30IJpyiaRxEZMzYtm0bOTk5FBQUKFyNQc45amtraWxsZOrUqd2OHWnwuoKV9K1uq69ibfo97HodXCfkTomErLKzdIWhiIxqhw8fpqKiotdEmjJ2pKenU1paSkpKSrf9ClYyOM01sPkZH7Le/yO0t0B6nh+PddKHNPhdRETGFE23IIOTVQin/o3f2pp9uNr0e3j3aVj7c0hOh2kf8CFr1mX+fBERkTFIwUqOTWpWZBHojnbY+VpkXNbmp/3g9ylnRLoM86fFu8UiIiLDRl2BEhvO+ZXfwyFrb7AKfPHsSCVr4qmQpFWURERkZNMYKxl++7cHUzg8BTv+7Ae/Z0/wk5LOuhymnQcpGfFupYiIyDFTsJL4OlgH7z3rx2RteR7aGv2kpNPO95WsmZdCTkm8WykiIjIgGrwu8ZWZD/M/6bf2Ntjxig9Z7z7jx2UBTF4EMy/zQatkjubLEhGREUkVK4kf56B6g+8ufPcZ2B38N5A7xVexZl0G5WdrsWgREUko6gqUkaFxL7z3B1/Nev8FaD8Eqdl+nqyZl8GMS7SOoYiIxN2gugLNbArwMFACOOB+59x3zCwfeBQoB7YDn3DO7Tc/7/93gMuBg8CNzrnVsfggMsrllMDC6/12+JBfLHpz0GW44TeRqRxmXeaDVuEMdRmKiEhCOWrFyswmAhOdc6vNLAdYBXwEuBGoc85908zuBMY7575sZpcDf48PVmcA33HOnXGk91DFSo6osxMq1/hK1uan/bQOAPknRga/ly3REjsiIjIsYtoVaGa/Af4j2M53zlUG4etF59wsM/thcH95cP674fP6e00FKzkmB3b5JXY2PwPbXoKONkjPhRMv9NM5TL9YXYYiIjJkYnZVoJmVA6cCbwAlUWGpCt9VCDAZ2BX1tIpgX7/BSuSY5E2B0z/jt9ZGv8TO5mf9+Kz1jwEGpafBzEtgxgdhwjx1GYqIyLAYcLAys2zg18DnnXMNFvUPlXPOmdkxlb7MbBmwDKCsrOxYnioSkZYDs6/yW2cnVL4VCVl//IbfcibBjIt9NWvqeZCWHe9Wi4jIKDWgrkAzSwF+B/zBOXdvsK+ri09dgZKQGvfCludg8x/8VYZtjRBK9VM4zPigr2hpLUMRETlGgxpjFVzl9xB+oPrno/Z/G6iNGrye75z7kpl9CLiNyOD17zrnTj/SeyhYyZBrb/MLRr/3rA9ate/5/QUzfCVrxiV+AHxyanzbKSIiCW+wweps4GVgHdAZ7P4qfpzVL4AyYAd+uoW6IIj9B3ApfrqFm5xzR0xNClYy7Oq2RroMt7/iB8Cn5sCJH4gErezieLdSREQSkCYIFTmS1ibY9id/leF7z0Fj0Gs9aWEkZE1cAElJcW2miIgkBgUrkYFyDqrWRqpZFSsBB1nFfgD89It8VStjfLxbKiIicaJgJXK8mmtgy4pgAPwfoeWAnwG+9DQ/X9aMi2DCfFWzRETGEAUrkVjoaIfdq/yVhltWwJ63/P6sIj856YyLYdoHNDmpiMgop2AlMhSa9sH7z/uQteV5OFQHGExeFOk2nHQqJIXi3VIREYkhBSuRodbZ4StYW1b4AfC7VwEOMvJh+oXB2KwLIbso3i0VEZFBUrASGW7NtbD1BR+ytqyAgzV+/6RTfciafrGvbGnhaBGREUfBSiSeOjuh6m14b4Ufn1XxF3CdkJ7nrzCcfrGvauVMiHdLRURkAGK2CLOIHIekJF+pmnQqnPd/4NB+v8TOlmB81vrH/XkT5gVdhhfAlDMgOS2+7RYRkWOmipVIPDkHVeuCAfArYNcb0NkOKVl+TcMTL/Bb4QyIWvhcRETiRxUrkURlBhNP8ds5X4CWBr/Ezvt/9FccvvcHf17uFN9teOIFMPU8yMyPb7tFRKRPClYiiSR9HJx0ud8A6rb5QfBbnof1T8Dqh/0EpZMWRqpZpYshlBLXZouIiKeuQJGRIjxBabiatXuVHwSfNg6mnhtUtC6E/KnxbqmIyKimqwJFRqND+2HbSz5obfkj1O/0+8dPjVSzpp7rq2AiIhIzClYio51zUPt+pJq17WU43AwWgimnB0HrQpi0QDPBi4gMkoKVyFjT3gYVbwbVrOeh8m3A+bmzpp0H08732/iputpQROQYKViJjHXNNbD1RR+0tr4IDbv9/ryySMiaeh5kFcavjSIiI4SClYhEOAe1W3zA2vqi7zZsrffHJsyLBK2ysyA1M37tFBFJUApWItK/jnbfVbj1BR+0dr0BHW0QSvUzwE87D6Z9ACYu0NqGIiIoWInIsWg7CDtfi1S0qtb6/Wm5fjb4cEVLs8GLyBilmddFZOBSM/2i0NMv9I+ba/y0Dltf9FWtd3/v9+dMioSsaedpEWkRERSsRORosgph7sf8BsFs8C/6bfPT8Pb/+P1FJ0eC1glnaf4sERmT1BUoIsevs9N3FYaD1s7XoL3Fz5816VQ/QenUc/1YLQ2EF5FRQmOsRGR4HG7xg9+3vQTbX/bL7nS2+4HwpadB+Tk+aJUuhuS0eLdWROS4DCpYmdlPgCuAaufc3GDfo8Cs4JQ84IBzboGZlQMbgXeDY6875245WgMVrERGqdZG2Pm6D1rbXopMVJqcAWVnBBWt83TFoYiMKIMdvP4g8B/Aw+Edzrlrol7834D6qPPfd84tOK6WisjokpYDMy72G/j1DXe8GgStl+H5f/L7U3P8uKyp58LUc6BkHiQlxa/dIiLH6ajByjn3UlCJ6sXMDPgEcEGM2yUio1HGeDjpQ34DaNrnuwy3v+zD1nt/iJxXfjaUB2O0imZpagcRGREGW3s/B9jrnHsvat9UM3sLaAC+5px7eZDvISKjVXZR9ysOG/b4Sla463Djb/3+rGJfyZp6rh+nlT9NQUtEEtJgg9W1wPKox5VAmXOu1swWAU+Y2RznXEPPJ5rZMmAZQFlZ2SCbISKjwrhJMP8avwHs3949aL3z6+C8Uh+0TljqK1vjyxW0RCQhDOiqwKAr8HfhwevBvmRgN7DIOVfRz/NeBP63c+6II9M1eF1Ejiq8xuG2PwVXHb4CB2v9sXGTIyGr/GxVtERkSA3VzOsXAZuiQ5WZFQF1zrkOM5sGzAC2DuI9REQ8M7+MTuEMOO1vfdDa964fn7Xjz34erXW/8OdmT4DyIGidcLaW3xGRYXPUYGVmy4HzgUIzqwDucs79GPgk3bsBAc4F/snMDgOdwC3OubrYNllEBB+Uik/y2+mf8UGr5j3Y8Qps/7MPW+Guw6xiH7TCVa2ikxS0RGRIaIJQERmdnIO6rb7LcMef/W3Dbn8ss9BP71B+tg9bxbM1vYOIDJgWYRaRsccMCk7026IbfNDavz0IWUHQ2vikPzdjvA9Y4YpWyVwFLRE5LgpWIjI2mEH+VL+d+jd+34GdkZC14xXY9Du/Pz0Xys4Kug/PggnzNTO8iAyI/qYQkbErrwwWlMGCa/3j+opgfFYwTmvz035/ShZMOc2HrROWwOTFWlRaRPqkMVYiIv1pqISdr/ltx2uw9x3AQVIKTFoAZUt8RWvKGZCZH+/WisgwGdQizMNBwUpERoRDB2DXm7DzVR+09qyGjjZ/rHh2JGiVLYHcyXFtqogMHQ1eFxGJhYw8mHmJ3wAOH4LdqyNBa+2jsPLH/lheWaTrsOwszaUlMkYoWImIHK+UjGAi0qX+cUc77F3nQ9bOV2HLClj7c38ssxDKzoxUtCacogHxIqOQftUiIrESSoZJp/ptyd9FluHZ8WowTuvVyJWHqdlQelokaE1epAHxIqOAgpWIyFCJXoZn0Q1+X8OeqKD1GrzwL/gB8cm+ilV2ph8MP+UMGDcxrs0XkWOnwesiIvF0aH8wIP512PUG7F4F7S3+WF4ZTDkTyoKgVTwbkkLxba+IaPC6iEjCyhgPMz/oN4D2NqhaB7te92Fr258ii0unjYPSxT5sTTnd30/LiV/bRaQXVaxERBKZc3BgB+x8w4etXW/C3vWAA0vyy+9Edx/mTYl3i0VGPc1jJSIymrTUQ8VfgrD1BlSshMPN/ti4yT5glQVVrZJ5uvpQJMbUFSgiMpqk58L0i/wGwTQP7/iQtesNH7jWP+aPpWRB6aKgonWmv58xPn5tFxnlFKxEREa6ULJfYmfSAjjjZr+vviISsna9Di/fC67DHyuc5ad6mHKavy06SYPiRWJEwUpEZDTKLfXb3P/lH7c2+SsOK/7it81Pw5qf+WOpOTB5YTAgPghbWvtQ5LgoWImIjAVp2TDtPL+BHxRft9WPz6p404et6KpW/omRKw9LT/dTPWislshR6VciIjIWmUHBiX6bf43f19YMe9YEQWslbHke3l7uj6Vk+apWOGiVngbZRXFrvkiiUrASEREvNav72ofOwYGdvpq1K6hqvfo96Gz3x8eXB12HQWVrwjwIpcSt+SKJQMFKRET6ZgbjT/DbvI/7fYcPQeXbkaC1/RVY90t/LDndr5NYutgHrsmL/PQPZvH7DCLDTPNYiYjI8XMOGnYHQSsYr1X5NnS0+ePZE3zACncjTjrVTxchMoJpHisRERkaZlFXIH7M72tvhap3/FWIu1fB7pXw7u8jzymcGYStYCuZC8mp8Wm/SIwpWImISGwlp/mJSEsXRfYd2g973oKKIGxtWREZGB9KhQmn+IpWOGzlT1MXooxI6goUEZHh55yfxHT3yqCqtdoHr8MH/fH0vO5VrcmLdBWiJIxBdQWa2U+AK4Bq59zcYN/dwGeAfcFpX3XOPRUc+wrwaaADuN0594dBfwIRERldzPyC0XlTYM5H/b6Odti3KaoLcRW8fA+4Tn88rywqaC2GifMhNTN+n0GkDwPpCnwQ+A/g4R7773PO3RO9w8xmA58E5gCTgBVmNtO58IxzIiIi/Qglw4S5flt0g9/X1uwHw+9eFQyOXwXrH/fHLOQnLp20wA+Kn7wQiudovJbE1VGDlXPuJTMrH+DrXQX83DnXCmwzsy3A6cBrx99EEREZs1Kz4ISz/BbWVO27DsPdiJt+B2/91B8LpULJHB+0wlvRSZpfS4bNYAav32Zm1wMrgS865/YDk4HXo86pCPb1YmbLgGUAZWVlg2iGiIiMKdnFMOtSv0EwkekOP0YrvK37Faz8iT+enO4nL520MBK2Cmdo4WkZEscbrP4L+GfABbf/BnzqWF7AOXc/cD/4wevH2Q4RERnrzPws8OPLI+O1Ojth/zYfssID49/6Gbz5Q388JcuP0QoHrckLYfxUSEqK16eQUeK4gpVzbm/4vpn9N/C74OFuYErUqaXBPhERkeGTlBRZCzE8a3xnB9S8172ytfLH0N7ij6flwqT53bsR807QtA9yTI4rWJnZROdcZfDwo8A7wf0ngf8xs3vxg9dnAG8OupUiIiKDlRSC4pP8tuBav6/jsL8SMTpsvfZ96Dzsj2eM7x60Js6H3CkKW9KvgUy3sBw4Hyg0swrgLuB8M1uA7wrcDtwM4Jxbb2a/ADYA7cCtuiJQREQSVijFj7+aMA8WXu/3tbdC9YaobsQ18Mq/Q/ifs4zxPmB1bQvUjShdNEGoiIjI0Rw+5JfpqXrbT/9Q+Tbs3RCpbKXmwMRTugeughl+CgkZdbRWoIiIyGCkZMCU0/wW1t4G+zZGglbl27DyAWg/5I8nZ/g5uaLDVtHJmmdrlFOwEhEROR7JqZHAFNbRDrXvdQ9bbz8Kf/mRP56UAiWzu3cjlszxwU1GBXUFioiIDKXw1A/RYatyjV+YGvwM8kWzule2JsyDtJy4Nlv6p65AERGReIme+mHux/y+8CLU0WHr/Rfg7eWR542fGgysPyUywH7cJF2RmOAUrERERIZb9CLUJ18R2d9YBZVrg8Hx66BqHWx8MnI8Iz8SssJb4Uwt2ZNAFKxEREQSRc4Ev828JLKvtdFfgVi11getqnV+zFZ4YtNQKhSf3L26VTIH0nPj8xnGOAUrERGRRJaWA2Vn+C2sox1qtwRBKwhc7z7jl+0Jyzuhd1dibqm6EoeYgpWIiMhIE0qOzCJ/ytV+n3O+K3HvO92rW5t+j5/PG0jPiwpbc4OuxFmaAiKGFKxERERGAzMYN9FvMy6O7G9t8jPJR4etlT+JzLeVlOKvSiyZE7XNhewSVbeOg4KViIjIaJaWDVNO91tYZwfUvh+ErbV+DNe2l2Hto5FzMvIjISscuIpOgtTM4f8MI4iClYiIyFiTFIKimX6b9/HI/oN1vrq1d73vUty7HlY/BIcPBieYnzYiOnAVz/bjubRWIqBgJSIiImGZ+VB+tt/CwhOc7l0fCVxV62DDk3SN3UrN9gErujuxeDZk5MXjU8SVgpWIiIj0L3qC09lXRva3NsG+TUFlK6hyrX8cVj0QOSd3SiRkhatcBdNH9eLUo/eTiYiIyNBJy4bSxX4Lcw4a9viQVb0+UuXasgI62/05oVR/JWLxycE229/mThkV3YkKViIiIhIbZpA72W/Rk5y2t0LNe5GuxOqNsONVWPeLyDmp2X5wfHTYKp4N2cUj6upEBSsREREZWslpwbxZc4FrIvtb6qF6kx8wvy+43fwMvPXTyDkZ+VFBKxy6ToKM8cP+MQZCwUpERETiIz2396zyAE37YN9GX9mq3uBv1z4KrQ2Rc3Im9qhunRxMB5E1vJ+hBwUrERERSSzZRX6bem5kn3PQsLt72Kre0H3dRAxO/jBc89M+X3Y4KFiJiIhI4jPzax3mlnafWb6zA/ZvD4LWRsgqjFsTQcFKRERERrKkUGQ6iJOviHdrGPnXNYqIiIgkCAUrERERkRhRsBIRERGJkaMGKzP7iZlVm9k7Ufu+bWabzGytmT1uZnnB/nIzO2Rma4LtB0PYdhEREZGEMpCK1YPApT32PQfMdc6dAmwGvhJ17H3n3IJguyU2zRQRERFJfEcNVs65l4C6Hvuedc4Fi/7wOlA6BG0TERERGVFiMcbqU8DTUY+nmtlbZvYnMzunvyeZ2TIzW2lmK/ft2xeDZoiIiIjE16CClZn9A9AOPBLsqgTKnHOnAl8A/sfMxvX1XOfc/c65xc65xUVFRYNphoiIiEhCOO5gZWY3AlcAf+2ccwDOuVbnXG1wfxXwPjAzBu0UERERSXjHFazM7FLgS8CVzrmDUfuLzCwU3J8GzAC2xqKhIiIiIonuqEvamNly4Hyg0MwqgLvwVwGmAc+ZGcDrwRWA5wL/ZGaHgU7gFudcXZ8vLCIiIjLKHDVYOeeu7WP3j/s599fArwfbKBEREZGRSDOvi4iIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMSIgpWIiIhIjChYiYiIiMTIgIKVmf3EzKrN7J2offlm9pyZvRfcjg/2m5l918y2mNlaM1s4VI0XERERSSQDrVg9CFzaY9+dwPPOuRnA88FjgMuAGcG2DPivwTdTREREJPENKFg5514C6nrsvgp4KLj/EPCRqP0PO+91IM/MJsagrSIiIiIJbTBjrEqcc5XB/SqgJLg/GdgVdV5FsE9ERERkVIvJ4HXnnAPcsTzHzJaZ2UozW7lv375YNENEREQkrgYTrPaGu/iC2+pg/25gStR5pcG+bpxz9zvnFjvnFhcVFQ2iGSIiIiKJYTDB6knghuD+DcBvovZfH1wdeCZQH9VlKCIiIjJqJQ/kJDNbDpwPFJpZBXAX8E3gF2b2aWAH8Ing9KeAy4EtwEHgphi3WURERCQhDShYOeeu7efQhX2c64BbB9MoERERkZFIM6+LiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMJMe7AcOhs9PxD0+so3R8JuUFWZxQkMkJBZnkpKfEu2kiIiIyioyJYFV3sI3nNlRT09TabX9hdionFGRxQn4mJxRkUV4Y3BZkkpeZGqfWioiIyEhlzrl4t4HFixe7lStXDvn7NLW2s6O2mR21B4Otme3B48r6lm7n5makcEJBJlPyM5kyPpPS8RmUjs9gSn4mk/MySE8JDXl7RUREJPGY2Srn3OK+jh13xcrMZgGPRu2aBnwdyAM+A+wL9n/VOffU8b5PLGWnJTNnUi5zJuX2OtZyuIOddb0D1/rd9Ty7vorDHd0DaFFOGlPGZ1AahK4p+eHwlcmkvHTSkhW8RERExpqYVKzMLATsBs4AbgKanHP3DPT5w1WxOl4dnY7qxhYq9h9iV91BKvYfomL/QXbVHaLiwEH2HGihozPy52gGJTnpvapck8dnMCkvg0m5GWSkKniJiIiMRENSserhQuB959wOM4vRSyaOUJIxMTeDibkZnFae3+t4e0cnVQ0tQeDqHr7+sn0/T769h84e+TU/K5VJeelMyvWBa3JeELryMpiUl05hVhpJSaPvz1JERGQ0i1Ww+iSwPOrxbWZ2PbAS+KJzbn+M3ichJYeSgi7BzD6PH+7opKq+hT0HDrGn/hC79x9i9wH/eFtNM69sqeFgW0e356SGkpiYl94tcE3OS4+EL1W9REREEs6guwLNLBXYA8xxzu01sxKgBnDAPwMTnXOf6uN5y4BlAGVlZYt27NgxqHaMZM45Gg61s/vAIfYcONTrds+BFvY2ttDzqwpXvSZ3BS9VvURERIbakboCYxGsrgJudc5d0sexcuB3zrm5R3qNRB9jlQiiq16R4BVUwYJ9fVW9JuSmMzHXV7om5qYzMS+DSbnpTMz14Ss3I4XR2H0rIiIyVIZ6jNW1RHUDmtlE51xl8PCjwDsxeI8xLyWU5Kd+yO+7u7G/qtee+hYqDxzizW11VDV0H2QPkJESYmIw1qtb8Iq6zU4bE9OdiYiIDNqg/sU0syzgYuDmqN3/amYL8F2B23sckyFiZuRmppCbmcLsSeP6PKej07GvsZU99YeoPNBCZb3vZqys9wFs8+Z97Gtq7dXlmJOe7INXXlDpCgLXxKhqmOb1EhERGWSwcs41AwU99l03qBbJkAklGRNy05mQmw5lfZ/T1t7J3oYWKut7BK/gdl1FPbXNbb2eNz4zpat7cWIQwsJVsEl5GZSMSyc1WUtTiojI6KY+HukmNfnIXY7gJ1Otqm/pXvkKuhwr9vtux4aW9m7PMYPC7LSu8V1dwSsvMt6rOCedkAbbi4jICKZgJccsPSVEeWEW5YVZ/Z7T3NreZ8Wrsr6F96obeem9fb0G24eSjJKctK5uxq4B91GVsIKsVF3pKCIiCUvBSoZEVloy04tzmF6c0+fx8GD7PfWHugevA74Stm53Pc9u2Etbe2e354WvdJyQm95tkP2E3MiYr/ysVF3pKCIicaFgJXERPdj+5Il9D7Z3zlHX3EZlMM1EZY/ux79s38/ehkrae1zpGB2+JnaFsIxujzXHl4iIDAUFK0lYZkZBdhoF2WnMndx74WzwVzrWNrUGg+194KoK7lfVt7B6536q6lt6LaKdEjJKxoWDlq92TQgeh7siC7PTNOZLRESOiYKVjGihJKN4XDrF49KZP6Xvczo7HbXNbUHgOkRVQyR47TlwiLUVB/jD+pZe3Y7hMV8TwgPuu6pgkepXcU4aySFd7SgiIp6ClYx6SUlGUU4aRTlpzCvtu/LlnGP/wcNd47wqG1qoCgbbV9W3sLGygec37aXlcPfwlWRQnNO923FijyBWMi6dFIUvEZExQcFKBN/tmJ+VSn5WKnMm9R++6g8d7gpb/tZPNVFV38LmvY38aXPvqx3DU01MjApdXUFsnL/6sXhcGmnJmmRVRGSkU7ASGSAzIy8zlbzM1CMOuG9sbe8aYB893quyoYWt+5p5dUstja3tvZ5bmJ3qB92PCy8vFA5fkeqXZrgXEUlsClYiMWRmjEtPYdyEFGZN6HuqCYDGlsNRM9y3UHmghaoG3/VYsf8gf9leR/2hw72eF57hfmK3qx4jU01MyE0nM1U/axGReNHfwCJxkJOeQk56Sr/zfAEcbGvv1e3YNeg+uOJx/8He4WtcenIw3UQGE8alMWFcOiVBt+OE4FZzfYmIDA0FK5EElZmazIlF2ZxYlN3vOeHlhSrrfcVrz4GWrkrY3oYWNlU29LmwdmooieJxftxXybhI6ApPQVEyLl3rO4qIHAcFK5ERbCDLC7V3dLIvmOtrb30LVQ3BFlS/3tldz4qNva94BCjISu2qcvWseoWD2Lj0ZFW/REQCClYio1xyKCkYl5XR7znhJYYqG/yA+70NLVTVt1IVPN5T38Jbuw5Q19zW67kZKaFI5atH1SscwopyNNmqiIwNClYi0m2JoZMm9H3FI/iux+qG1q6q1976SLdjVUMLb26ro7qx90z34fm+fNUrLah4ZTAhN61bV6QG3ovISKe/xURkwNJTQpQVZFJWkNnvOeGZ7veGuxujbveGp5x4v5bGlt5TToQH3ofD1sTcSBdkuBKmgfciksgUrEQkpqJnuu9vjUeA5tb2rqpXVdSA+3BX5LtVjdQ0tdLZz8D76HFfJeN85as4x99X9UtE4kV/84hIXGSlHf2qx/DA+3DYqqxv6RbGNuxp4I8bqzl0uKPXc3PSkikOAldkS+u6Lc5J14z3IhJzClYikrAGOvC+qbWdvQ2tVIfHfjW0srehhepGf7+/sV8A+VmpFOek9Qhe3YNYQVaqFtsWkQFRsBKREc3MoiZc7b/6FV5oe28w1is8CH9vEMSqG1vYVNXAvsbe3Y9JBkU53bsb+wpi4zNTNP5LZIxTsBKRMSF6oe3+1noE3/0YHnwfrnztjQpgFfsPsnrn/j6nnkgNJVGUkxYMwE8LQljPAJZGdprm/hIZrRSsRESiJIeSukLQkbS2+6knwt2NXZWvoDvy3apGXt5c0+eC25mpoaD6ldZ1FWSkOzISyjJSNf5LZKRRsBIROQ5pySGm5GcyJb//qSfAX/1Y3egH4PsQFjUGrKGVt3YeYG9DC63tvWe+z0lLpmhcGiXBQPvinMig+6Ko+zmqgIkkjEEHKzPbDjQCHUC7c26xmeUDjwLlwHbgE865/YN9LxGRkSYrLZmpaclMPcKyQ+GZ7/c2RqacqG5sZV+jr4iFA1h1Y0ufSw+lpyT5kJWTFgSw9KgxYZF9GgMmMvRiVbH6gHOuJurxncDzzrlvmtmdweMvx+i9RERGleiZ72eW5PR7nnOOhpZ29gVhqzoqeIXvbzpCF2RKyCjKTqMoHLiiql7R93UVpMjxG6quwKuA84P7DwEvomAlIjIoZkZuRgq5GSlML+4/gAEcbGvvN3zta2xlZ+1BVm6vY//Bw72em2RQkJ3WZ/gq6nZf84CJ9BSLYOWAZ83MAT90zt0PlDjnKoPjVUBJDN5HREQGKDM1mfLCZMqP0AUJfhB+TVMb1cHYr32NvhsyPDC/urGVd/Y0UNvHLPgAeZkpkfCVk0Zxj+7H8H3NhC9jRSz+Sz/bObfbzIqB58xsU/RB55wLQlc3ZrYMWAZQVlYWg2aIiMixSksOMTkvg8l5/U/CCn4airrmtq6ql78CMhK+qhtb2bqviX1NrX1OxJqVGupa6qgw298WhW+jtoKsNFKT1Q0pI5c518f/ghzvi5ndDTQBnwHOd85VmtlE4EXn3Kz+nrd48WK3cuXKmLVDRETio7PTceDQ4W7dj/vCW5OviIUfN/SxEDfA+MyUSNjqEb6iQ9n4zFSSkjQYX4afma1yzi3u69igKlZmlgUkOecag/uXAP8EPAncAHwzuP3NYN5HRERGhqSkyESsJ0048rkthzuoafIhq6apLSqARcLXqp37qW5o7XM6ilCSUZid2juAZfuxYNGBLCs1pCsiZVgMtiuwBHg8+I81Gfgf59wzZvYX4Bdm9mlgB/CJQb6PiIiMMukpIUrHZ1I6/shzgYXXgwyHLR/CWoIKWKQatqGygZqmNjr6GAyWkRIKKl6pUQEsvVdXZGF2qgbky6AMKlg557YC8/vYXwtcOJjXFhERge7rQU4r6n89SPBdkfsPtnWFrpro8BUEsG01zby5re8rIgFyM1J6VcEKs33o8rdpFOakajyY9EmXaYiIyKiRlGQUZKdRkJ121K7ItvZOapu7B6+uIBbcrq04wL7GVprbOvp8jXHpyRSGx35lp1EQHb6yUykI9hfmpOrKyDFC37KIiIxJqclJTMzNYGLuka+IBD8vWE1jGzXNrdQE3ZE1Ta3UNgVdk02tbKxqoLapjfpDfVfCMlNDPYJX9ypY+FhRdhrjMrRM0UilYCUiInIUmanJlBUkU1Zw5PFgEKmEHSmI7ao7yFs791PX3Nbn/GApIaMgy1e6ooNXUVQoCwex/KxUQro6MmEoWImIiMTQsVTCOoIxYTVNQRBrag22yP3apjberWqkpp85wpIM8rNS+wxi0V2UBdl+uaL0FA3OH0oKViIiInHip4zwQYijjAkLL9bdVxVsX1QQe2vnAWqaWjnYz7iwrNRQMA4tlYIgkOWH72cHj7Mi1TAN0D82ClYiIiIjQPRi3Sce5epI6D0urK65jdpmH8DqmtuobWpj94EW1lbUU9fcRntffZJATnpyV8jqGb6iuyMLslPJz9QC3gpWIiIio9CxjAsLV8Nqm1upDUJXbXMrdU3dw9iO2oOs3nmAuua+144Ev35kflYqhVm+KuZDV1pXKIuuho3PHH3jwxSsRERExrjoati0oqOf39npqD902AexIHz5QBaphtU2t7Kluona5jb2H2yjrxX0zCA/M7Wr4lXQVxjrCmRp5GWkJPwyRgpWIiIickySkozxWamMz0plevHRzw8P0q/r0RXZM4xtqmqgtrmNA/1M3ppkMD7Tv2+4azK/x3ZCQRYLpuTF9gMfAwUrERERGVLRg/RnluQc9fzDHZ3sP+jDVjiM7W9u6xonFr59r7qJuh4VsQ/MKuKBm04f4k/UPwUrERERSSgpoSSKc9Ipzkkf0PkdQddkXXNr3CdWVbASERGRES2UZF1dgfE2tq+JFBEREYkhBSsRERGRGFGwEhEREYkRBSsRERGRGFGwEhEREYkRBSsRERGRGFGwEhEREYkRBSsRERGRGFGwEhEREYkRBSsRERGRGDEXXrUwno0w2wfsGIa3KgRqhuF9ZOD0nSQmfS+JSd9L4tF3kpiG+ns5wTlX1NeBhAhWw8XMVjrnFse7HRKh7yQx6XtJTPpeEo++k8QUz+9FXYEiIiIiMaJgJSIiIhIjYy1Y3R/vBkgv+k4Sk76XxKTvJfHoO0lMcftextQYKxEREZGhNNYqViIiIiJDZkwEKzO71MzeNbMtZnZnvNszlpjZFDN7wcw2mNl6M/tcsD/fzJ4zs/eC2/HBfjOz7wbf1VozWxjfTzB6mVnIzN4ys98Fj6ea2RvBn/2jZpYa7E8LHm8JjpfHteGjmJnlmdmvzGyTmW00syX6rcSfmd0R/P31jpktN7N0/V6Gn5n9xMyqzeydqH3H/PswsxuC898zsxti3c5RH6zMLAT8J3AZMBu41sxmx7dVY0o78EXn3GzgTODW4M//TuB559wM4PngMfjvaUawLQP+a/ibPGZ8DtgY9fhbwH3OuenAfuDTwf5PA/uD/fcF58nQ+A7wjHPuJGA+/vvRbyWOzGwycDuw2Dk3FwgBn0S/l3h4ELi0x75j+n2YWT5wF3AGcDpwVziMxcqoD1b4P7gtzrmtzrk24OfAVXFu05jhnKt0zq0O7jfi/6GYjP8OHgpOewj4SHD/KuBh570O5JnZxOFt9ehnZqXAh4AfBY8NuAD4VXBKz+8k/F39CrgwOF9iyMxygXOBHwM459qccwfQbyURJAMZZpYMZAKV6Pcy7JxzLwF1PXYf6+/jg8Bzzrk659x+4Dl6h7VBGQvBajKwK+pxRbBPhllQEj8VeAMocc5VBoeqgJLgvr6v4fHvwJeAzuBxAXDAOdcePI7+c+/6ToLj9cH5EltTgX3AA0EX7Y/MLAv9VuLKObcbuAfYiQ9U9cAq9HtJFMf6+xjy381YCFaSAMwsG/g18HnnXEP0MecvTdXlqcPEzK4Aqp1zq+LdFukmGVgI/Jdz7lSgmUi3BqDfSjwE3URX4YPvJCCLGFc4JDYS5fcxFoLVbmBK1OPSYJ8MEzNLwYeqR5xzjwW794a7LYLb6mC/vq+htxS40sy247vGL8CP7ckLujqg+59713cSHM8FaoezwWNEBVDhnHsjePwrfNDSbyW+LgK2Oef2OecOA4/hf0P6vSSGY/19DPnvZiwEq78AM4IrOFLxgw6fjHObxoxgbMGPgY3OuXujDj0JhK/GuAH4TdT+64MrOs4E6qPKvBIDzrmvOOdKnXPl+N/DH51zfw28AHw8OK3ndxL+rj4enB/3/yscbZxzVcAuM5sV7LoQ2IB+K/G2EzjTzDKDv8/C34t+L4nhWH8ffwAuMbPxQTXykmBfzIyJCULN7HL8mJIQ8BPn3P+Lb4vGDjM7G3gZWEdkPM9X8eOsfgGUATuATzjn6oK/uP4DX2o/CNzknFs57A0fI8zsfOB/O+euMLNp+ApWPvAW8DfOuVYzSwd+ih8fVwd80jm3NU5NHtXMbAH+goJUYCtwE/5/gPVbiSMz+0fgGvxVzm8Bf4sfl6PfyzAys+XA+UAhsBd/dd8THOPvw8w+hf93COD/OeceiGk7x0KwEhERERkOY6ErUERERGRYKFiJiIiIxIiClYiIiEiMKFiJiIiIxIiClYiIiEiMKFiJyKhgZp83s8x4t0NExjZNtyAio0Iwk/xi51xNvNsiImOXKlYiMuKYWZaZ/d7M3jazd8zsLvw6bi+Y2QvBOZeY2WtmttrMfhmsV4mZbTezfzWzdWb2pplNj+dnEZHRRcFKREaiS4E9zrn5zrm5+JUV9gAfcM59wMwKga8BFznnFgIrgS9EPb/eOTcPPzPzvw9ry0VkVFOwEpGRaB1wsZl9y8zOcc7V9zh+JjAb+LOZrcGvIXZC1PHlUbdLhrqxIjJ2JB/9FBGRxOKc22xmC4HLgW+Y2fM9TjHgOefctf29RD/3RUQGRRUrERlxzGwScNA59zPg28BCoBHICU55HVgaHj8VjMmaGfUS10TdvjY8rRaRsUAVKxEZieYB3zazTuAw8Fl8l94zZrYnGGd1I7DczNKC53wN2BzcH29ma4FWoL+qlojIMdN0CyIypmhaBhEZSuoKFBEREYkRVaxEREREYkQVKxEREZEYUbASERERiREFKxEREZEYUbASERERiREFKxEREZEYUbASERERiZH/Hy5cpEJXys25AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(RawData,RawC)=LireDonnees(\"Cotations2021.csv\")\n",
    "(D,C)=CreerData(RawData,RawC)\n",
    "(weights,constant, nbstep)=Algorithme_Gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Construction du prédicteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input: weights, les poids appris pendant l'apprentissage\n",
    "#       constant, la constante apprise pendant l'apprentissage\n",
    "#       v, le vecteur à partir duquel on veut prédire\n",
    "# Nécessite : que l'apprentissage ait été fait, i.e. on dispose de weights et constant\n",
    "# Output : le résultat de la prédiction\n",
    "def Prediction(weights, constant, v):\n",
    "    if Sigmoid(H(weights, constant, v))>0.5:\n",
    "            return 1\n",
    "    else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Généralisons ce prédicteur pour l'évaluer sur une base de vecteurs $\\vec v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input: weights, les poids appris pendant l'apprentissage\n",
    "#       constant, la constante apprise pendant l'apprentissage\n",
    "#       D, une base de données de vecteurs à partir duquel on veut prédire\n",
    "# Nécessite : que l'apprentissage ait été fait, i.e. on dispose de weights et constant\n",
    "# Output : Cpred, le vecteur des prédictions associées\n",
    "\n",
    "def PredictionsOnBase(weigths,constant,D):\n",
    "    Cpred=[]\n",
    "    for index in range(len(D)):\n",
    "        Cpred.append(Prediction(weights,constant,D[index:index+1,:]))\n",
    "    return Cpred"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation du prédicteur sur la base d'apprentissage\n",
    "\n",
    "Le code ci-dessous permet d'évaluer la qualité du prédicteur sur la base d'apprentissage, autrement dit:\n",
    "- Calcul du pourcentage de bonnes classifications,\n",
    "- Matrice de confusion : vous donnera la répartition des bonnes/mauvaises classifications."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de bonnes classifications :  94.8051948051948 %\n",
      "Quand il fallait prédire 1...\n",
      "94.69026548672566 % du temps le prédicteur prédit 1\n",
      "5.3097345132743365 % du temps le prédicteur prédit 0\n",
      "Quand il fallait prédire 0...\n",
      "94.91525423728814 % du temps le prédicteur prédit 0\n",
      "5.084745762711864 % du temps le prédicteur prédit 1\n"
     ]
    }
   ],
   "source": [
    "Cpred=PredictionsOnBase(weights,constant,D)\n",
    "GoodPred=0\n",
    "VraiPositifs=0\n",
    "FauxPositifs=0\n",
    "VraiNegatifs=0\n",
    "FauxNegatifs=0\n",
    "NumberPositif=0\n",
    "for index in range(len(C)):\n",
    "    if (C[index]==1): \n",
    "        NumberPositif +=1\n",
    "    if (C[index]==Cpred[index]):\n",
    "        GoodPred+=1\n",
    "    if (C[index]==1):\n",
    "        if (Cpred[index]==1):\n",
    "            VraiPositifs+=1\n",
    "        else:\n",
    "            FauxNegatifs+=1\n",
    "    if (C[index]==0):\n",
    "        if (Cpred[index]==1):\n",
    "            FauxPositifs+=1\n",
    "        else:\n",
    "            VraiNegatifs+=1\n",
    "print(\"Taux de bonnes classifications : \",100.0*GoodPred/len(C),\"%\")\n",
    "print(\"Quand il fallait prédire 1...\")\n",
    "print(100.0*VraiPositifs/NumberPositif,\"% du temps le prédicteur prédit 1\")\n",
    "print(100.0*FauxNegatifs/NumberPositif,\"% du temps le prédicteur prédit 0\")\n",
    "print(\"Quand il fallait prédire 0...\")\n",
    "print(100.0*VraiNegatifs/(len(C)-NumberPositif),\"% du temps le prédicteur prédit 0\")\n",
    "print(100.0*FauxPositifs/(len(C)-NumberPositif),\"% du temps le prédicteur prédit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}