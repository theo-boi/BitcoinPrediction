{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction l'évolution du cours du Bitcoin\n",
    "\n",
    "Nous y voilà : vous allez devoir travailler maintenant pour proposer votre propre algorithme de descente pour résoudre un problème de classification tout simple. Je vous donne l'historique sur 2020 et 2021 du cours du Bitcoin et vous devez construire un prédicteur capable de détecter si le cours du Bitcoin va partir à la hausse ou à la baisse. \n",
    "\n",
    "Vous serez évaluez sur la qualité du travail réalisé et surtout sur les résultats que vous obtiendrez sur des données que vous ne connaissez pas. En fonction de vos résultats par rapport aux autres binômes vous obtiendrez plus ou moins de points. \n",
    "\n",
    "Voyons un peu plus les choses en détails.\n",
    "\n",
    "## Définition du sujet\n",
    "\n",
    "### Le cours du Bitcoin\n",
    "\n",
    "Dans les fichiers Cotations2021.csv et Cotations2020.csv vous trouverez l'évolution journalière du cours du Bitcoin en 2021 et en 2020. Ces fichiers contiennent 5 colonnes : \n",
    " - Date : jour de la cotation,\n",
    " - Ouverture : valeur en euros du cours du Bitcoin à l'ouverture du marché,\n",
    " - Plus Haut : valeur en euros la plus élevée du Bitcoin durant le jour considéré,\n",
    " - Plus Bas : valeur en euros la plus basse du Bitcoin durant le jour considéré,\n",
    " - Clôture : valeur en euros du cours du Bitcoin à la clôture du marché.\n",
    "\n",
    "La notion de \"cours du Bitcoin\" est assez vague : finalement, on se pose vraiment la question de savoir combien vaut le Bitcoin au moment où on doit en acheter ou en vendre... Pour simplifier un peu, nous considérerons que d'un jour $t$ à un jour $(t+1)$ le cours du Bitcoin augmente (diminue) si sa valeur à la clôture au jour $t$ est inférieure (supérieure) à sa valeur à la clôture au jour $(t+1)$. \n",
    "\n",
    "Autrement dit, on va regarder la colonne \"Clôture\" pour décider de l'augmentation ou la baisse du cours du Bitcoin. \n",
    "\n",
    "### Enoncé du problème de classification\n",
    "\n",
    "Le problème que vous avez à résoudre est donc un problème de classification : pour un jour $t$ donné, vous voulez prédire si à $(t+1)$ le cours va augmenter (classe 1) ou diminuer (classe 2). Le cas, marginal, ou le cours reste constant sera assimilé à la classe 2.\n",
    "\n",
    "Pour réaliser cette prédiction, vous pouvez prendre en compte l'historique du cours sur les $h$ derniers jours. Vous aurez toute liberté de définir cette notion d'historique : c'est d'ailleurs un des facteurs qui pourra influencer la qualité de votre prédicteur. Quelle valeur de $h$ considérer ? Quelle(s) donnée(s) considérer sur les jours précédents : uniquement la valeur de clôture ou ... ? \n",
    "\n",
    "### Démarche pour réaliser ce travail\n",
    "\n",
    "Déjà, vous pouvez fortement vous inspirer de ce que l'on a fait dans la première partie des Travaux Pratiques, ainsi que sur le cours (notamment, le chapitre \"Liens avec le Machine Learning\", partie sur l'exemple de la société ERAM). \n",
    "\n",
    "Au niveau démarche, c'est toujours la même chose : \n",
    "1. Définir la notion d'historique : quel vecteur de données allez vous prendre en compte pour l'apprentissage et la prédiction ensuite ? Notons $\\vec v$ ce vecteur.\n",
    "1. Définir le problème d'optimisation à résoudre (fonction $H(\\vec v)$ de séparation des données, fonction d'activation (sigmoide), fonction de perte/erreur),\n",
    "2. Définir le gradient,\n",
    "3. Créer votre algorithme de descente,\n",
    "\n",
    "Pour ce dernier point, vous pourrez faire appel à toutes les techniques/éléments vus dans le chapitre \"Optimisation continue sans contraintes\" quand nous avons parlé des algorithmes de descente.\n",
    "\n",
    "En plus du code Python ci-dessus, il faudra produire du code de traitement des données et du code pour que je puisse mettre en oeuvre l'évaluation de votre projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Développement de votre algorithme de descente\n",
    "\n",
    "### Outils conseillés\n",
    "- La bibliothèque [pandas](https://pandas.pydata.org/docs/user_guide/index.html) est idéale pour l'utilisation d'une base de donnée.\n",
    "- La bibliothèque [numpy](https://numpy.org/doc/1.18/user/index.html) est parfaite pour tout calcul sur des vecteurs.\n",
    "- La bibliothèque [matplotlib](https://matplotlib.org/contents.html) est la base pour visualiser des résultats mais accouplé à [seaborn](https://seaborn.pydata.org/index.html) il est possible d'aller plus vite.\n",
    "- Le bibliothèque [math](https://docs.python.org/fr/3.9/library/math.html) est souvent oubliée tellement elle semble évidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import math\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données\n",
    "\n",
    "La première tâche à réaliser va être de développer une fonction qui va lire un fichhier .csv contenant un historique de cotations et créer une base panda. La structure $RawData$ va contenir les données telles que contenues dans le fichier .csv passé en paramètre (selon la syntaxe décrite précédemment). De même, $RawC$ va contenir l'appartenance aux classes 1 (diminution) / 0 (augmentation).\n",
    "\n",
    "Vous utiliserez les structures $RawData$ et $RawC$ comme vous le souhaiterez par la suite dans vos algorithmes : par contre, ces structures sont à considérer comme des données d'entrée dans toute la suite des TPs.\n",
    "\n",
    "Autrement dit, pour évaluer vos algorithmes je serais amené à leur donner de nouveaux $RawData$ et $RawC$ contenant des données que vous ne connaissez pas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: nom du fichier .csv à lire.\n",
    "#Nécessite :les lignes du fichier sont classées par dates de cotatation croissantes et les colonnées séparées par \";\"\n",
    "#Output: RawData, contient l'historique des cotations telles que lues dans le .csv\n",
    "#        RawC, contient pour chaque ligne/date t (sauf la dernière) 1 si le cours baisse à t+1; 0 sinon.\n",
    "def LireDonnees(NomFichier):\n",
    "    #Lecture des données dans le fichier csv\n",
    "    RawData=pd.read_csv(NomFichier, sep=';')\n",
    "    \n",
    "    #Construction des classes d'appartenance de chaque ligne de la base: on suppose que les cotations journalières sont\n",
    "    #classées par ordre chronologique\n",
    "    RawC=[]\n",
    "    for index in range(len(RawData)-1):\n",
    "        value=RawData.iloc[index,4] > RawData.iloc[index+1,4]\n",
    "        value=1*value\n",
    "        RawC.append(value)\n",
    "    return RawData, RawC\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des données utilisées pour l'apprentissage/prédiction\n",
    "\n",
    "Nous allons maintenant définir une méthode qui va préparer une structure de données, nommée $data$, qui va contenir pour chaque ligne un couple :\n",
    "\n",
    "$\\vec v=(\\vec x ; c_x)$\n",
    "\n",
    "où $\\vec x$ est le vecteur caractérisant la ligne en cours. Prenons un exemple, que vous adapterez comme vous le souhaitez par la suite. Je décide que pour la ligne $t$, date de cotation $t$, ce qui explique la prédiction $c$ (baisse/augmentation de la valeur de la cotation à $t+1$) c'est le cours du Bitcoin à la clôture sur les $h=3$ derniers jours. Donc, la ligne d'indice 0 dans $data$ va contenir :\n",
    "\n",
    "$(x_1,x_2,x_3, c_x=0$ ou $1)$ \n",
    "\n",
    "avec $x_j$ la valeur de cotation à la clôture le jour $j$.\n",
    "\n",
    "Créez la fonction CreerData ci-dessous pour avoir par la suite des données plus faciles à manipuler grâce à numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11139.82728087649\n"
     ]
    }
   ],
   "source": [
    "#Input: RawData et RawC ont été créés à partir d'un fichier .csv\n",
    "#Output: D, contient les points $(\\vec x)$. Cela doit être une matrice numpy\n",
    "#        C, contient la classe d'appartenance $c_x$ de chaque entrée dans data. Cela doit être un vecteur numpy\n",
    "def CreerData(RawData,RawC):\n",
    "    h = 3 # On prend les 3 derniers jours\n",
    "    variables = (\"Ouverture\", \"Plus Haut\", \"Plus Bas\", \"Cloture\") # On sélectionne les variables désirées\n",
    "    norm = RawData[RawData.columns[1:len(variables)+1]].stack().mean()\n",
    "    print(norm)\n",
    "\n",
    "    nbDates = len(RawData.index)\n",
    "    \n",
    "    D = np.empty((nbDates, len(variables)))\n",
    "    for t in range(nbDates): # On va instancier chaque v\n",
    "        for xVar in range(len(variables)): # et pour chaque x_t, chaque variable\n",
    "            D[t][xVar] = RawData[variables[xVar]][t] / norm\n",
    "\n",
    "    C = np.array(RawC)\n",
    "\n",
    "    return(D,C)\n",
    "\n",
    "(RawData,RawC)=LireDonnees(\"Cotations2020.csv\")\n",
    "(D,C)=CreerData(RawData,RawC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du problème d'optimisation à résoudre\n",
    "\n",
    "On a choisit\n",
    "$\\vec v=(\\vec x ; c_x) \\in R ^{ h \\times 4 }, \\ avec \\ \\vec x=(y_1, y_2, y_3, y_4), \\ \\vec y$ étant l'ensemble des valeurs des variables sélectionnées\n",
    "\n",
    "$H(\\vec v) = \\displaystyle\\sum_{i=1}^{h} \\vec w_i \\cdot \\vec x_i + b, \\ avec \\ h = 3 \\ et \\ \\forall i \\in \\{1..h\\}, \\ \\vec w_i \\in R^4$\n",
    "\n",
    "D'où $H(\\vec v) = \\vec w_1 \\cdot \\vec x_1 + \\vec w_2 \\cdot \\vec x_2 + \\vec w_3 \\cdot \\vec x_3 + b$\n",
    "\n",
    "On va utiliser la fonction logistique sigmoide pour transformer les valeurs de $H(\\vec v)$ en valeurs $0$ ou $1$:\n",
    "\n",
    "$\\mathcal{S}: x \\mapsto \\frac{1}{1+\\exp^{-\\lambda x}}$.\n",
    "\n",
    "Notre classificateur est donc : $\\mathcal{S}(H(\\vec v))$.\n",
    "\n",
    "Pour notre problème, on cherche à apprendre sur la base d'apprentissage les valeurs de $\\vec w_i \\ \\forall i \\in \\{1..h\\}$ et $b$.\n",
    "\n",
    "On choisit la cross-entropy pour mesurer l'erreur de notre problème, c'est à dire la distance entre $\\mathcal{S}(H(\\vec v))$ et $c_{\\vec v}$ :\n",
    "\n",
    "$E(\\mathcal{P}) = \\sum \\limits_{\\vec v \\in \\mathcal{P}}-(c_{\\vec v}\\log_2(\\mathcal{S}(H(\\vec v)))+(1-c_{\\vec v})\\log_2(1-\\mathcal{S}(H(\\vec v))))$.\n",
    "\n",
    "$\\mathcal{P} : \\Bigg\\{$\n",
    "$\\  Minimiser \\ E(P)$\n",
    "$\\ \\ s.c.$\n",
    "$\\ \\ \\vec w_i \\in R^4 \\ et \\ b \\in R$\n",
    "$\\Bigg\\}$\n",
    "\n",
    "Commençons par définir le code qui calcule $H(\\vec v)$ puis le code de la fonction logistique sigmoid $\\cal{S}(x)$. Je suppose que vous allez reprendre les mêmes fonctions que dans le TP introductif, mais si vous voulez changer vous pouvez, bien sûr ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la méthode pour calculer H(v)\n",
    "def H(weights, constant, point):\n",
    "    value=np.sum(weights*point)+constant\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons maintenant le code qui calcule $\\mathcal{S}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction sigmoide (logistique) et la fonction d'arrondie\n",
    "def Sigmoid(x, lamda=0.000005):\n",
    "        return 1/(1+np.exp(-lamda*x))\n",
    "\n",
    "\n",
    "def Step_function(x):\n",
    "        return 1*(x>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à la définition de la fonction d'erreur $E(\\mathcal{P})$, qui est notre fonction à minimiser, ainsi que du gradient. Je vous laisse les définir !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la méthode pour calculer l'erreur \n",
    "#Input : weights, le vecteur de poids utilisés dans la fonction H,\n",
    "#        constant, la constante utilisée dans la fonction H\n",
    "#Output : une valeur numérique, l'erreur commise. \n",
    "def Loss_function(weights, constant):\n",
    "    loss_value = 0\n",
    "    for index in range(len(D)-3):\n",
    "        p = D[index:index+3,:] # shape : (3, 4)\n",
    "        c = C[index]\n",
    "        #print(c, np.log2(Sigmoid(H(weights, constant, p))))\n",
    "        #print((1-c), np.log2(1-Sigmoid(H(weights, constant, p))))\n",
    "        loss_value -= c*np.log2(Sigmoid(H(weights, constant, p)))+(1-c)*np.log2(1-Sigmoid(H(weights, constant, p)))\n",
    "        #print(loss_value)\n",
    "        #print()\n",
    "    #print(loss_value)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition du calcul de gradient\n",
    "#Input : weights, le vecteur de poids utilisés dans la fonction H,\n",
    "#        constant, la constante utilisée dans la fonction H\n",
    "#Output : le gradient, un numpy array\n",
    "def Gradient(weights, constant):\n",
    "    grad = []\n",
    "    # Looping over all variables (number of variables=number of weights)\n",
    "    for w in range(len(weights)):\n",
    "        coord_value=0\n",
    "        for index in range(len(D)-3):\n",
    "            x=D[index:index+3,:]\n",
    "            coord_value-= x[0,w]*(C[index]-Sigmoid(H(weights, constant, x)))\n",
    "        grad.append(coord_value)\n",
    "    coord_value=0\n",
    "    # Considering the last component=constant\n",
    "    for index in range(len(D)-3):\n",
    "        x=D[index:index+3,:]\n",
    "        coord_value-= (C[index]-Sigmoid(H(weights, constant, x)))\n",
    "    grad.append(coord_value)\n",
    "    \n",
    "    return np.array(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme de descente\n",
    "\n",
    "<<Expliquer ici quels sont les caractéristiques/paramètres de votre algorithme de descente>>\n",
    "\n",
    "<<Justifiez les choix retenus et les choix testés>>\n",
    "\n",
    "Note: lorsque vous coderez votre algorithme de gradient, il faut que celui-ci affiche dans un graphique l'évolution de la valeur de la fonction d'erreur/loss et l'évolution de la norme du gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre algorithme de gradient qui va travailler à partir des structures numpy D et C\n",
    "# Input :\n",
    "# Nécessite :\n",
    "# Output : weights, un numpy array contenant les poids utilisés dans la fonction de séparation H \n",
    "#          constant, un numérique contenant la constante utilisée dans la fonction de séparation H\n",
    "#          nbstep, nombre d'itérations mises par l'algorithme avant de s'arrêter\n",
    "\n",
    "# ATTENTION: j'ai laissé dans l'algorithme vide ci-dessous des lignes de code permettant l'affichage de la fonction de perte/loss\n",
    "#            et la norme du gradient. Gardez ce code qui vous aidera à voir si votre algorithme converge ou pas.\n",
    "\n",
    "def Algorithme_Gradient():\n",
    "    \n",
    "   #Initialisations pour les graphiques terminaux: à conserver\n",
    "    ## Produire deux graphes cote à cote 2 subplots\n",
    "    fig, ax = plt.subplots(figsize=(10, 5) , nrows=1, ncols=1)\n",
    "    ## Pour visualiser les evolutions en fonction du step\n",
    "    itersteps = []\n",
    "    ## Pour visualiser l'evolution de la norme du gradient\n",
    "    iterGap = []\n",
    "    ## Pour visualiser l'évolution de l'erreur\n",
    "    iterError = []\n",
    "    ## Pour visualiser la séparation linéaire\n",
    "    #line_X = np.array([min(data[\"x\"]), max(data[\"x\"])])\n",
    "    \n",
    "    #Initialisation de l'algorithme du gradient \n",
    "    ## initialiser les poids et la constante\n",
    "    weights = np.random.random(np.shape(D)[1])\n",
    "    constant = rd.random()\n",
    "    ## initialiser le learning rate\n",
    "    learning_rate = 0.1\n",
    "    #Initialisation du pas\n",
    "    nbstep = 20\n",
    "    step = 1 # numéro d'itération dans l'algorithme\n",
    "    ## On dessine la droite de séparation des données avec les poids/constante initiaux\n",
    "    #line_Y = -(weights[0]*line_X + constant)/weights[1]\n",
    "    #ax[0].plot(line_X, line_Y, label='initial step')  \n",
    "\n",
    "    # Exécution de l'algorithme de gradient\n",
    "    while step < nbstep:\n",
    "        \n",
    "        # Calcule de la fonction de perte/erreur\n",
    "        E = Loss_function(weights,constant)\n",
    "        #print(E)\n",
    "        #print()\n",
    "        # Calcule du gradient et de sa norme\n",
    "        total_grad = Gradient(weights,constant)\n",
    "        norm = np.linalg.norm(total_grad)\n",
    "        # Mise à jour des poids et de la constante\n",
    "        weights-=(learning_rate*total_grad[:-1])/norm\n",
    "        constant-=learning_rate*total_grad[-1]/norm\n",
    "\n",
    "        # Stockage de valeurs à cette itération pour affichage final\n",
    "        itersteps.append(step - 1)\n",
    "        iterGap.append(norm)\n",
    "        iterError.append(E)\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "\n",
    "    # Affichage final\n",
    "    #line_Y = -(weights[0]*line_X + constant)/weights[1]\n",
    "    #ax[0].plot(line_X, line_Y, label='step '+str(step))\n",
    "    #sb.scatterplot(ax = ax[0], data=data, x=\"x\", y=\"y\", hue=\"class\")\n",
    "    #ax[0].legend()\n",
    "    #ax[0].set_xlabel('x')\n",
    "    #ax[0].set_ylabel('y')\n",
    "    print(iterGap)\n",
    "    # Affichage final : à conserver\n",
    "    ax.plot(itersteps,iterGap, label='Norme du gradient')\n",
    "    ax.plot(itersteps,iterError, label='Erreur')\n",
    "    ax.set_xlabel('step')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return(weights,constant,nbstep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase d'apprentissage \n",
    "\n",
    "Allez, on se lance ! Exécutez votre algorithme d'apprentissage pour voir ce qu'il donne. Je vous met ci-dessous un code générique que vous pouvez légèrement adapter. Néanmoins, faîtes bien attention que : \n",
    "* La fonction LireDonnees() doit être appelée pour lire des données brutes dans un fichier .csv qui est votre base d'apprentissage,\n",
    "* La fonction CréerData() doit être appelée pour créer les structures $D$ et $C$ qui sont utilisées par votre algorithme de gradient,\n",
    "* Aprés vous appelez l'algorithme de gradient que vous avez vous-même définis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47772.571166666676\n",
      "[3.5675692815517777, 3.5673250851621416, 3.5670809079067514, 3.566836749787491, 3.566592610806276, 3.5663484909649825, 3.5661043902654965, 3.5658603087097145, 3.565616246299536, 3.5653722030368282, 3.565128178923502, 3.5648841739614383, 3.5646401881525365, 3.564396221498667, 3.5641522740017297, 3.5639083456636156, 3.5636644364862207, 3.563420546471413, 3.5631766756211047]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(RawData,RawC)=LireDonnees(\"Cotations2021.csv\")\n",
    "(D,C)=CreerData(RawData,RawC)\n",
    "(weights,constant, nbstep)=Algorithme_Gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du prédicteur\n",
    "\n",
    "Il faut maintenant construire votre prédicteur : je vous propose le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: weights, les poids appris pendant l'apprentissage\n",
    "#       constant, la constante apprise pendant l'apprentissage\n",
    "#       v, le vecteur à partir duquel on veut prédire\n",
    "# Nécessite : que l'apprentissage ait été fait, i.e. on dispose de weights et constant\n",
    "# Output : le résultat de la prédiction\n",
    "def Prediction(weights, constant, v):\n",
    "    if Sigmoid(H(weights, constant, v))>0.5:\n",
    "            return 1\n",
    "    else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Généralisons un peu ce prédicteur pour l'évaluer sur une base de vecteurs $\\vec v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: weights, les poids appris pendant l'apprentissage\n",
    "#       constant, la constante apprise pendant l'apprentissage\n",
    "#       D, une base de données de vecteurs à partir duquel on veut prédire\n",
    "# Nécessite : que l'apprentissage ait été fait, i.e. on dispose de weights et constant\n",
    "# Output : Cpred, le vecteur des prédictions associées\n",
    "\n",
    "def PredictionsOnBase(weigths,constant,D):\n",
    "    Cpred=[]\n",
    "    for index in range(len(D)):\n",
    "        Cpred.append(Prediction(weights,constant,D[index:index+1,:]))\n",
    "    return Cpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du prédicteur sur la base d'apprentissage\n",
    "\n",
    "Le code ci-dessous va vous permettre d'évaluer la qualité de votre prédicteur sur la base d'apprentissage, autrement dit:\n",
    "- Calcul du pourcentage de bonnes classifications,\n",
    "- Matrice de confusion : vous donnera la répartition des bonnes/mauvaises classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de bonnes classifications :  51.181102362204726 %\n",
      "Quand il fallait prédire 1...\n",
      "0.0 % du temps le prédicteur prédit 1\n",
      "100.0 % du temps le prédicteur prédit 0\n",
      "Quand il fallait prédire 0...\n",
      "100.0 % du temps le prédicteur prédit 0\n",
      "0.0 % du temps le prédicteur prédit 1\n"
     ]
    }
   ],
   "source": [
    "# Pour exécuter ce code vous devez avoir au préalable exécuté l'apprentissage => on doit disposer des weights et constant\n",
    "\n",
    "Cpred=PredictionsOnBase(weights,constant,D)\n",
    "GoodPred=0\n",
    "VraiPositifs=0\n",
    "FauxPositifs=0\n",
    "VraiNegatifs=0\n",
    "FauxNegatifs=0\n",
    "NumberPositif=0\n",
    "for index in range(len(C)):\n",
    "    if (C[index]==1): \n",
    "        NumberPositif +=1\n",
    "    if (C[index]==Cpred[index]):\n",
    "        GoodPred+=1\n",
    "    if (C[index]==1):\n",
    "        if (Cpred[index]==1):\n",
    "            VraiPositifs+=1\n",
    "        else:\n",
    "            FauxNegatifs+=1\n",
    "    if (C[index]==0):\n",
    "        if (Cpred[index]==1):\n",
    "            FauxPositifs+=1\n",
    "        else:\n",
    "            VraiNegatifs+=1\n",
    "print(\"Taux de bonnes classifications : \",100.0*GoodPred/len(C),\"%\")\n",
    "print(\"Quand il fallait prédire 1...\")\n",
    "print(100.0*VraiPositifs/NumberPositif,\"% du temps le prédicteur prédit 1\")\n",
    "print(100.0*FauxNegatifs/NumberPositif,\"% du temps le prédicteur prédit 0\")\n",
    "print(\"Quand il fallait prédire 0...\")\n",
    "print(100.0*VraiNegatifs/(len(C)-NumberPositif),\"% du temps le prédicteur prédit 0\")\n",
    "print(100.0*FauxPositifs/(len(C)-NumberPositif),\"% du temps le prédicteur prédit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
